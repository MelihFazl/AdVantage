{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9E0Lj_tGzD1"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfuYr5k5G1QY",
        "outputId": "694de50a-031e-43a0-a719-d48ebb257011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install ijson\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import ijson\n",
        "import json\n",
        "import gzip\n",
        "import ijson\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ads_dataset_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads.json'\n",
        "ads_downsized_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_downsized.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGrHo_MILle"
      },
      "source": [
        "DATA DOWNSIZING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59BCGxSBINGp",
        "outputId": "1f67efcc-d839-4e67-ff13-9c88e0bd7d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "767264 items [05:44, 2227.86 items/s]                        \n"
          ]
        }
      ],
      "source": [
        "json_item_heuristic = 767263\n",
        "counter = 0\n",
        "\n",
        "with open(ads_dataset_path, 'r') as input_file:\n",
        "    parser = ijson.items(input_file, 'item')\n",
        "\n",
        "    with open(ads_downsized_path, 'a') as output_file:  # Open output file once\n",
        "        output_file.write('[\\n')\n",
        "        # Use tqdm to create a progress bar\n",
        "        for item in tqdm(parser, total=int(json_item_heuristic), unit=\" items\"):\n",
        "            if counter > json_item_heuristic:\n",
        "                break\n",
        "            counter += 1\n",
        "            json.dump(item, output_file)\n",
        "            if counter > json_item_heuristic:\n",
        "              output_file.write('\\n')  # Add a newline between items\n",
        "            else:\n",
        "              output_file.write(',\\n')  # Add a newline between items\n",
        "\n",
        "        output_file.write(']\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA DUPLICATE REMOVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'\n",
        "\n",
        "def remove_duplicates(filename, column, output_filename):\n",
        "    json_item_heuristic = 767263\n",
        "    first_item = True\n",
        "    removed_content = 0\n",
        "    unique_values = set()\n",
        "\n",
        "    with open(filename, 'rb') as file:\n",
        "        items = ijson.items(file, 'item')\n",
        "\n",
        "        with open(output_filename, 'w') as outfile:\n",
        "            outfile.write('[\\n')\n",
        "            for item in tqdm(items, total=int(json_item_heuristic), unit=\" items\"):                    \n",
        "                if 'ad_creative_bodies' in item:\n",
        "                    column_value = tuple(item['ad_creative_bodies'])\n",
        "\n",
        "                    if column_value not in unique_values:\n",
        "                        unique_values.add(column_value)\n",
        "                        if first_item:\n",
        "                            json.dump(item, outfile)\n",
        "                            first_item = False\n",
        "                        else:\n",
        "                            outfile.write(',\\n')\n",
        "                            json.dump(item, outfile)\n",
        "                    else:\n",
        "                        removed_content += 1\n",
        "                else:\n",
        "                    removed_content += 1\n",
        "                    pass\n",
        "            outfile.write(']\\n')\n",
        "    print(f\"Amount of ads that have been filtered: {removed_content}\")\n",
        "\n",
        "\n",
        "remove_duplicates(ads_downsized_path, 'a', ads_non_duplicate_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "JSON TO CSV CONVERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'\n",
        "ads_dataframe = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_dataframe.csv'\n",
        "\n",
        "# Load JSON data\n",
        "with open(ads_non_duplicate_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Process JSON data and keep certain attributes\n",
        "processed_data = []\n",
        "for item in data:\n",
        "    # Keep only certain attributes, modify as needed\n",
        "    selected_attributes = {'ad_creation_time': item['ad_creation_time'], \n",
        "                           'ad_creative_bodies': item['ad_creative_bodies'], \n",
        "                           'currency': item['currency'],\n",
        "                           'impressions': item['impressions'],\n",
        "                           'spend': item['spend']}\n",
        "    processed_data.append(selected_attributes)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(ads_dataframe, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATAFRAME INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(ads_dataframe)\n",
        "print(df.dtypes)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERT INTO DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "# Extract relevant values into new DataFrame\n",
        "new_df = pd.DataFrame()\n",
        "\n",
        "new_df['ad_creation_time'] = pd.to_datetime(df['ad_creation_time'])\n",
        "new_df['ad_creative_bodies'] = df['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)\n",
        "\n",
        "\n",
        "# Extract 'lower_bound' and 'upper_bound' values if they exist, otherwise use NaN\n",
        "new_df['impressions_lower'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['impressions_upper'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "new_df['spend_lower'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['spend_upper'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(new_df.dtypes)\n",
        "\n",
        "new_df.head()\n",
        "\n",
        "ads_dataframe = pd.DataFrame()\n",
        "ads_dataframe['ad_creation_time'] = new_df['ad_creation_time']\n",
        "ads_dataframe['ad_creative_bodies'] = new_df['ad_creative_bodies']\n",
        "ads_dataframe['cpi'] = (\n",
        "    (new_df['spend_lower'].fillna(new_df['spend_upper']) / 2 +\n",
        "     new_df['spend_upper'].fillna(new_df['spend_lower']) / 2) /\n",
        "    (new_df['impressions_lower'].fillna(new_df['impressions_upper']) / 2 +\n",
        "     new_df['impressions_upper'].fillna(new_df['impressions_lower']) / 2)\n",
        ")\n",
        "ads_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAIN ML MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "X = ads_dataframe['ad_creative_bodies'].astype(str)\n",
        "y = ads_dataframe['cpi']\n",
        "\n",
        "# Tokenize your text data\n",
        "tokenized_text = X.apply(word_tokenize)\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "pt_y = PowerTransformer(method='box-cox')\n",
        "y_transformed = pt_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tokenized_text, y_transformed, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to average word vectors for a document\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "# Function to generate document vectors using average word vectors\n",
        "def generate_doc_vectors(docs, model, num_features):\n",
        "    doc_vectors = [average_word_vectors(doc, model, model.wv.index_to_key, num_features) for doc in docs]\n",
        "    return np.array(doc_vectors)\n",
        "\n",
        "# Generate document vectors for training and testing sets\n",
        "X_train_word2vec = generate_doc_vectors(X_train, word2vec_model, 100)\n",
        "X_test_word2vec = generate_doc_vectors(X_test, word2vec_model, 100)\n",
        "\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train_word2vec, y_train)\n",
        "\n",
        "predictions_transformed = gb_regressor.predict(X_test_word2vec)\n",
        "\n",
        "predictions = pt_y.inverse_transform(predictions_transformed.reshape(-1, 1)).flatten()\n",
        "\n",
        "mse_original = mean_squared_error(y_test, predictions)\n",
        "r2_original = r2_score(y_test, predictions)\n",
        "\n",
        "print(f\"Mean Squared Error (Original): {mse_original}\")\n",
        "print(f\"R-squared (Original): {r2_original}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
