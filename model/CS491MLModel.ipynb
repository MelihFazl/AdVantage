{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9E0Lj_tGzD1"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfuYr5k5G1QY",
        "outputId": "694de50a-031e-43a0-a719-d48ebb257011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install ijson\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import ijson\n",
        "import json\n",
        "import gzip\n",
        "import ijson\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ads_dataset_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads.json'\n",
        "ads_downsized_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_downsized.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGrHo_MILle"
      },
      "source": [
        "DATA DOWNSIZING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59BCGxSBINGp",
        "outputId": "1f67efcc-d839-4e67-ff13-9c88e0bd7d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "767264 items [05:44, 2227.86 items/s]                        \n"
          ]
        }
      ],
      "source": [
        "json_item_heuristic = 3000000\n",
        "\n",
        "def downsize_data(num_of_items, input_dataset_path, output_dataset_path):\n",
        "    counter = 0\n",
        "    with open(input_dataset_path, 'r') as input_file:\n",
        "        parser = ijson.items(input_file, 'item')\n",
        "\n",
        "        with open(output_dataset_path, 'a') as outfile:  # Open output file once\n",
        "            outfile.write('[\\n')\n",
        "            # Use tqdm to create a progress bar\n",
        "            for item in tqdm(parser, total=int(num_of_items), unit=\" items\"):\n",
        "                if counter > num_of_items:\n",
        "                    break\n",
        "                counter += 1\n",
        "                json.dump(item, outfile)\n",
        "                if counter > num_of_items:\n",
        "                  outfile.write('\\n')  # Add a newline between items\n",
        "                else:\n",
        "                  outfile.write(',\\n')  # Add a newline between items\n",
        "\n",
        "            outfile.write(']\\n')\n",
        "            outfile.close()\n",
        "\n",
        "downsize_data(json_item_heuristic, ads_dataset_path, ads_downsized_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA DUPLICATE REMOVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'\n",
        "\n",
        "def remove_duplicates(filename, column, output_filename):\n",
        "    json_item_heuristic = 3000\n",
        "    first_item = True\n",
        "    removed_content = 0\n",
        "    unique_values = set()\n",
        "\n",
        "    with open(filename, 'rb') as file:\n",
        "        items = ijson.items(file, 'item')\n",
        "\n",
        "        with open(output_filename, 'w') as outfile:\n",
        "            outfile.write('[\\n')\n",
        "            for item in tqdm(items, total=int(json_item_heuristic), unit=\" items\"):\n",
        "                if 'ad_creative_bodies' in item:\n",
        "                    column_value = tuple(item['ad_creative_bodies'])\n",
        "\n",
        "                    if (column_value not in unique_values) and (len(column_value) == 1):\n",
        "                        unique_values.add(column_value)\n",
        "                        if first_item:\n",
        "                            json.dump(item, outfile)\n",
        "                            first_item = False\n",
        "                        else:\n",
        "                            outfile.write(',\\n')\n",
        "                            json.dump(item, outfile)\n",
        "                    else:\n",
        "                        removed_content += 1\n",
        "                else:\n",
        "                    removed_content += 1\n",
        "                    pass\n",
        "            outfile.write(']\\n')\n",
        "    print(f\"Amount of ads that have been filtered: {removed_content}\")\n",
        "\n",
        "\n",
        "remove_duplicates(ads_downsized_path, 'a', ads_non_duplicate_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "JSON TO CSV CONVERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'\n",
        "ads_dataframe = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_dataframe.csv'\n",
        "\n",
        "with open(ads_non_duplicate_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "processed_data = []\n",
        "for item in data:\n",
        "    selected_attributes = {'ad_creation_time': item['ad_creation_time'],\n",
        "                           'ad_creative_bodies': item['ad_creative_bodies'],\n",
        "                           'currency': item['currency'],\n",
        "                           'impressions': item['impressions'],\n",
        "                           'spend': item['spend']}\n",
        "    processed_data.append(selected_attributes)\n",
        "\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "df.to_csv(ads_dataframe, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATAFRAME INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(ads_dataframe)\n",
        "print(df.dtypes)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERT INTO DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "new_df = pd.DataFrame()\n",
        "\n",
        "new_df['ad_creation_time'] = pd.to_datetime(df['ad_creation_time'])\n",
        "new_df['ad_creative_bodies'] = df['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)\n",
        "\n",
        "\n",
        "# Extract 'lower_bound' and 'upper_bound' values if they exist, otherwise use NaN\n",
        "new_df['impressions_lower'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['impressions_upper'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "new_df['spend_lower'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['spend_upper'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "print(new_df.dtypes)\n",
        "\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MERGE COLUMNS AND SORT ACCORDING TO DATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ads_dataframe = pd.DataFrame()\n",
        "ads_dataframe['ad_creation_time'] = new_df['ad_creation_time']\n",
        "ads_dataframe['ad_creative_bodies'] = new_df['ad_creative_bodies']\n",
        "ads_dataframe['cpi'] = (\n",
        "    (new_df['spend_lower'].fillna(new_df['spend_upper']) / 2 +\n",
        "     new_df['spend_upper'].fillna(new_df['spend_lower']) / 2) /\n",
        "    (new_df['impressions_lower'].fillna(new_df['impressions_upper']) / 2 +\n",
        "     new_df['impressions_upper'].fillna(new_df['impressions_lower']) / 2)\n",
        ")\n",
        "\n",
        "ads_dataframe = ads_dataframe.sort_values(by='ad_creation_time')\n",
        "ads_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VECTORIZE TEXTUAL CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "def generate_doc_vectors(docs, model, num_features):\n",
        "    doc_vectors = [average_word_vectors(doc, model, model.wv.index_to_key, num_features) for doc in docs]\n",
        "    return np.array(doc_vectors)\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Determine the split index\n",
        "split_index = int(0.8 * len(ads_dataframe))\n",
        "\n",
        "train_data = ads_dataframe.iloc[:split_index]\n",
        "test_data = ads_dataframe.iloc[split_index:]\n",
        "\n",
        "X_train = train_data['ad_creative_bodies'].astype(str)\n",
        "y_train = train_data['cpi']\n",
        "\n",
        "X_test = test_data['ad_creative_bodies'].astype(str)\n",
        "y_test = test_data['cpi']\n",
        "\n",
        "# Tokenization and stopword removal\n",
        "tokenized_text_train = X_train.apply(tokenize_and_remove_stopwords)\n",
        "tokenized_text_test = X_test.apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Word2Vec model training\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text_train, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate document vectors\n",
        "X_train_word2vec = generate_doc_vectors(tokenized_text_train, word2vec_model, 100)\n",
        "X_test_word2vec = generate_doc_vectors(tokenized_text_test, word2vec_model, 100)\n",
        "\n",
        "#word2vec_model.save(\"word2vec_model.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAIN ML MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model training and evaluation\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train_word2vec, y_train)\n",
        "\n",
        "predictions = gb_regressor.predict(X_test_word2vec)\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(f\"Mean Squared Error (Original): {mse}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
