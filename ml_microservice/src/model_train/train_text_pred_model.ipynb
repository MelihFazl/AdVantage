{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTORIZE TEXTUAL CONTENT FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "def generate_doc_vectors(docs, model, num_features):\n",
    "    doc_vectors = [average_word_vectors(doc, model, model.wv.index_to_key, num_features) for doc in docs]\n",
    "    return np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Determine the split index\n",
    "split_index = int(0.8 * len(ads_dataframe))\n",
    "\n",
    "train_data = ads_dataframe.iloc[:split_index]\n",
    "test_data = ads_dataframe.iloc[split_index:]\n",
    "\n",
    "X_train = train_data['ad_creative_bodies'].astype(str)\n",
    "y_train = train_data['cpi']\n",
    "\n",
    "X_test = test_data['ad_creative_bodies'].astype(str)\n",
    "y_test = test_data['cpi']\n",
    "\n",
    "# Tokenization and stopword removal\n",
    "tokenized_text_train = X_train.apply(tokenize_and_remove_stopwords)\n",
    "tokenized_text_test = X_test.apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Word2Vec model training\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate document vectors\n",
    "X_train_word2vec = generate_doc_vectors(tokenized_text_train, word2vec_model, 100)\n",
    "X_test_word2vec = generate_doc_vectors(tokenized_text_test, word2vec_model, 100)\n",
    "\n",
    "word2vec_model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save('/content/drive/MyDrive/CS491MLMODEL/us/us/word2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN ML MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and evaluation\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_regressor.fit(X_train_word2vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gb_regressor, '/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')\n",
    "#loaded_model = joblib.load('/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD MODEL (RUN IF YOU DO NOT WANT TO TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_regressor = joblib.load('/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
