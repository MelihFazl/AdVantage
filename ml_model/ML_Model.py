# -*- coding: utf-8 -*-
"""CS491MLModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H7EJ8UF9ECYRPzJIAVMRM3pmvfhQahIc

# **PREPROCESSING DATA FOR BERT REGRESSION**

INSTALLS
"""

!pip install ijson
!pip install shap
!pip install pdpbox

shap.__version__

"""IMPORTS"""

import re
import pandas as pd
import os
from tqdm import tqdm
import ijson
from sklearn.feature_extraction.text import TfidfVectorizer
import shap
from pdpbox import pdp
import joblib
import json
import gzip
import pdpbox
import json
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import ndcg_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from google.colab import drive
import pandas as pd
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import PowerTransformer
import nltk
import ast
from nltk.corpus import stopwords
drive.mount('/content/drive')


ads_dataset_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads.json'
ads_downsized_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_downsized.json'
ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'
ads_dataframe = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_dataframe.csv'
ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'
ads_dataframe_age_filtered = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_dataframe_age_filtered.csv'

"""*DATA* DOWNSIZING"""

json_item_heuristic = 3000000

def downsize_data(num_of_items, input_dataset_path, output_dataset_path):
    counter = 0
    with open(input_dataset_path, 'r') as input_file:
        parser = ijson.items(input_file, 'item')

        with open(output_dataset_path, 'a') as outfile:  # Open output file once
            outfile.write('[\n')
            # Use tqdm to create a progress bar
            for item in tqdm(parser, total=int(num_of_items), unit=" items"):
                if counter > num_of_items:
                    break
                counter += 1
                json.dump(item, outfile)
                if counter > num_of_items:
                  outfile.write('\n')  # Add a newline between items
                else:
                  outfile.write(',\n')  # Add a newline between items

            outfile.write(']\n')
            outfile.close()

downsize_data(json_item_heuristic, ads_dataset_path, ads_downsized_path)

"""DATA DUPLICATE REMOVAL"""

# @title
def remove_duplicates(filename, column, output_filename):
    json_item_heuristic = 3000
    first_item = True
    removed_content = 0
    unique_values = set()

    with open(filename, 'rb') as file:
        items = ijson.items(file, 'item')

        with open(output_filename, 'w') as outfile:
            outfile.write('[\n')
            for item in tqdm(items, total=int(json_item_heuristic), unit=" items"):
                if 'ad_creative_bodies' in item:
                    column_value = tuple(item['ad_creative_bodies'])

                    if (column_value not in unique_values) and (len(column_value) == 1):
                        unique_values.add(column_value)
                        if first_item:
                            json.dump(item, outfile)
                            first_item = False
                        else:
                            outfile.write(',\n')
                            json.dump(item, outfile)
                    else:
                        removed_content += 1
                else:
                    removed_content += 1
                    pass
            outfile.write(']\n')
    print(f"Amount of ads that have been filtered: {removed_content}")


remove_duplicates(ads_downsized_path, 'a', ads_non_duplicate_path)

"""JSON TO CSV CONVERSION"""

with open(ads_non_duplicate_path, 'r') as file:
    data = json.load(file)

processed_data = []
for item in data:
    selected_attributes = {'ad_creation_time': item['ad_creation_time'],
                           'ad_creative_bodies': item['ad_creative_bodies'],
                           'currency': item['currency'],
                           'impressions': item['impressions'],
                           'spend': item['spend']}
    processed_data.append(selected_attributes)

df = pd.DataFrame(processed_data)

df.to_csv(ads_dataframe, index=False)

"""
LOAD DATAFRAME & GET INFORMATION"""

df = pd.read_csv(ads_dataframe)
print(df.dtypes)
df.head()

"""FEATURE EXTRACTION FROM DATAFRAME"""

new_df = pd.DataFrame()

new_df['ad_creation_time'] = pd.to_datetime(df['ad_creation_time'])
new_df['ad_creative_bodies'] = df['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)


# Extract 'lower_bound' and 'upper_bound' values if they exist, otherwise use NaN
new_df['impressions_lower'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype("float")
new_df['impressions_upper'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype("float")

new_df['spend_lower'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype("float")
new_df['spend_upper'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype("float")

print(new_df.dtypes)

new_df.head()

"""CHECK FOR DUPLICATES"""

#Should print 0
new_df.duplicated(keep = False).sum()

"""DROP DUPLICATES FOR AD CREATIVE BODIES AND REMOVE NAN VALUES FOR"""

new_df.drop_duplicates(subset = "ad_creative_bodies", keep=False, inplace=True)
columns_to_check = ['impressions_lower', 'impressions_upper', 'spend_lower', 'spend_upper']
new_df = new_df.dropna(subset=columns_to_check)

"""CHECK MISSING VALUE RATES IN FEATURES"""

new_df.isna().sum()/len(df)

"""MERGE COLUMNS AND SORT ACCORDING TO DATE"""

ads_dataframe = pd.DataFrame()
ads_dataframe['ad_creation_time'] = new_df['ad_creation_time']
ads_dataframe['ad_creative_bodies'] = new_df['ad_creative_bodies']
ads_dataframe['spend'] = (new_df['spend_lower'].fillna(new_df['spend_upper']) / 2 +
     new_df['spend_upper'].fillna(new_df['spend_lower']) / 2)
ads_dataframe['impressions'] = (new_df['impressions_lower'].fillna(new_df['impressions_upper']) / 2 +
     new_df['impressions_upper'].fillna(new_df['impressions_lower']) / 2)

ads_dataframe = ads_dataframe.sort_values(by='ad_creation_time')
ads_dataframe

"""SEPERATE TRAIN AND TEST"""

# Determine the split index
split_value = 0.1
split_index = int(split_value * len(ads_dataframe))

train_data = ads_dataframe.iloc[:split_index]
test_data = ads_dataframe.iloc[split_index:]

"""CHECK FOR OUTLIERS"""

continuous_numerical = [
    'spend',
    'impressions'
    ]

train_data[continuous_numerical].describe([0.1, 0.25, 0.5, 0.75, 0.9,
                                 0.99])

"""REMOVE OUTLIERS (SINCE THE MAX IS MUCH HIGHER THAN THE 99TH PERCENTILE, AND VICE VERSA)"""

def remove_outliers(df, lower_outliers, q_bottom, upper_outliers,
                    q_top):
    lower_quantiles = df[lower_outliers].quantile(q_bottom)
    for col in lower_outliers:
        df = df[df[col] >= lower_quantiles[col]]
    upper_quantiles = df[upper_outliers].quantile(q_top)
    for col in upper_outliers:
        df = df[df[col] <= upper_quantiles[col]]
    return df

upper_outliers = ['spend', 'impressions']
lower_outliers = ['spend', 'impressions']
train_data = remove_outliers(train_data, lower_outliers, 0.01, upper_outliers, 0.99)

"""SKEWNESS ANALYSIS"""

#train_data.drop(columns=['ad_creation_time', 'ad_creative_bodies']).skew().sort_values(ascending=False)

"""LOG TRANSFORMATION FOR SKEWED DATA"""

# Apply log transformation only to specific columns
train_data['spend'] = np.log(train_data['spend'])
train_data['impressions'] = np.log(train_data['impressions'])

"""# **PREDICTION WITH SUPERVISED LEARNING**

VECTORIZE TEXTUAL CONTENT
"""

def tokenize_and_remove_stopwords(text):
  # Remove URLs from the text
    text = re.sub(r'http\S+', '', text)

    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]
    return tokens

def average_word_vectors(words, model, vocabulary, num_features):
    feature_vector = np.zeros((num_features,), dtype="float64")
    nwords = 0.

    for word in words:
        if word in vocabulary:
            nwords = nwords + 1.
            feature_vector = np.add(feature_vector, model.wv[word])

    if nwords:
        feature_vector = np.divide(feature_vector, nwords)

    return feature_vector

def generate_doc_vectors(docs, model, num_features):
    doc_vectors = [average_word_vectors(doc, model, model.wv.index_to_key, num_features) for doc in docs]
    return np.array(doc_vectors)

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

X_train = train_data['ad_creative_bodies'].astype(str)
y_train = train_data['cpi']

X_test = test_data['ad_creative_bodies'].astype(str)
y_test = test_data['cpi']

# Tokenization and stopword removal
tokenized_text_train = X_train.apply(tokenize_and_remove_stopwords)
tokenized_text_test = X_test.apply(tokenize_and_remove_stopwords)

# Word2Vec model training
word2vec_model = Word2Vec(sentences=tokenized_text_train, vector_size=100, window=5, min_count=1, workers=4)

# Generate document vectors
X_train_word2vec = generate_doc_vectors(tokenized_text_train, word2vec_model, 100)
X_test_word2vec = generate_doc_vectors(tokenized_text_test, word2vec_model, 100)

word2vec_model.save("word2vec_model.model")

"""SAVE WORD2VEC MODEL"""

word2vec_model.save('/content/drive/MyDrive/CS491MLMODEL/us/us/word2vec_model.model')

"""TRAIN ML MODEL"""

# Model training and evaluation
gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_regressor.fit(X_train_word2vec, y_train)

"""SAVE MODEL"""

joblib.dump(gb_regressor, '/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')
#loaded_model = joblib.load('/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')

"""LOAD MODEL (RUN IF YOU DO NOT WANT TO TRAIN)"""

gb_regressor = joblib.load('/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')

"""
TEST WITH NDCG"""

# Function to create subsets of data
def create_data_subsets(data, group_size):
    grouped_data = [data.iloc[i : i + group_size] for i in range(0 , data.shape[0], group_size)]
    return grouped_data

def calculate_ndcg_for_groups(groups, model):
    ndcg_scores = []
    for group in tqdm(groups, total=len(groups), unit=" items"):
        tokenized_X_train_group = group['ad_creative_bodies'].apply(tokenize_and_remove_stopwords)
        X_train_word2vec_group = generate_doc_vectors(tokenized_X_train_group, word2vec_model, 100)

        y_group_pred = model.predict(X_train_word2vec_group)

        # Assuming lower CPI values are more relevant
        relevance_scores = 1 / y_group_pred

        # Normalize relevance scores (optional)
        #relevance_scores = (relevance_scores - np.min(relevance_scores)) / (np.max(relevance_scores) - np.min(relevance_scores))

        y_group_true = 1 / group['cpi'].values

        try:
            ndcg = ndcg_score([y_group_true], [relevance_scores])
            ndcg_scores.append(ndcg)
        except Exception as e:
            print(y_group_true)
            print(relevance_scores)

    return ndcg_scores

group_size = 4
test_data_subsets = create_data_subsets(test_data, group_size)

ndcg_scores = calculate_ndcg_for_groups(test_data_subsets, gb_regressor)

"""PRINT NDCG SCORES"""

# Combine NDCG scores across groups (simple average in this example)
average_ndcg_across_groups = sum(ndcg_scores) / len(ndcg_scores)

print(f"Average NDCG Across Groups: {round(average_ndcg_across_groups,2)}")

"""# **RECOMMENDATION MODEL**"""

# Determine the split index
split_index = int(0.8 * len(ads_dataframe))

train_data = ads_dataframe.iloc[:split_index]
test_data = ads_dataframe.iloc[split_index:]

# Function to remove URLs
def remove_urls(text):
    # Use regular expressions to remove URLs and keywords with case insensitivity
    text = re.sub(r'\bsmartnews\b', '', text, flags=re.IGNORECASE)  # Whole word match for 'smartnews'
    text = re.sub(r'\bnews\b', '', text, flags=re.IGNORECASE)       # Whole word match for 'news'
    return re.sub(r'http\S+', '', text)


# Vectorize the text data using TF-IDF
max_features = 2000  # Choose the top most important words
vectorizer = TfidfVectorizer(max_features = max_features, stop_words='english')
X_train_suggestion = vectorizer.fit_transform(train_data['ad_creative_bodies'].apply(remove_urls))
X_test_suggestion = vectorizer.transform(test_data['ad_creative_bodies'].apply(remove_urls))

# Train a regression model
gb_regressor_suggestion = GradientBoostingRegressor(n_estimators=200, learning_rate=0.001, random_state=42, loss="absolute_error")
gb_regressor_suggestion.fit(X_train_suggestion, train_data['cpi'])

"""GRID SEARCH (RUN WHEN YOU WANT TO TEST POSSIBLE PARAMETERS)"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import make_scorer, mean_squared_error
import numpy as np

# Example feature and target arrays (X, y)
# X, y = your_data.features, your_data.target

# Setting up parameter grid
param_grid = {
    'loss': ['absolute_error'],
    'n_estimators': [200, 500],
    'learning_rate': [0.01, 0.001],
}

# Create a GradientBoostingRegressor object
gb_regressor = GradientBoostingRegressor(random_state=42)

# Set up the grid search with cross-validation
grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=5, verbose=1)

# Fit grid search
grid_search.fit(X_train_suggestion, train_data['cpi'])

# Best parameters and best score
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score (MSE):", -grid_search.best_score_)

"""PARTIAL DEPENDENCE PLOT"""

# Choose a feature for the Partial Dependence Plot (e.g., the first feature)
feature_index = 1
feature_name = vectorizer.get_feature_names_out()[feature_index]

# Create the Partial Dependence Plot.
pdp_plotter = pdpbox.pdp.PDPIsolate(gb_regressor_suggestion, pd.DataFrame.sparse.from_spmatrix(X_test_suggestion, columns = vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out(), feature_name, feature_name, n_classes=0)
pdp_plot = pdp_plotter.plot()
pdp_plot[0]

pdp_plot[0]

"""SHAP VALUE"""

import matplotlib.pyplot as plt
import shap

def shap_value_plot(feature_names, shap_values_instance):
    # Plotting
    plt.figure(figsize=(10, 5))
    plt.bar(feature_names, shap_values_instance[0], color='lightblue')
    plt.title('SHAP values for instance index {}'.format(instance_index))
    plt.xlabel('Feature')
    plt.ylabel('SHAP value')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

joblib.dump(gb_regressor_suggestion, '/content/drive/MyDrive/CS491MLMODEL/us/us/gb_regressor_suggestion.joblib')

feature_names = vectorizer.get_feature_names_out()

explainer = shap.TreeExplainer(gb_regressor_suggestion)

instance_index = 469  # Index of the instance in the test set

single_instance = X_test_suggestion[instance_index].toarray()

print(single_instance)
print(test_data['ad_creative_bodies'][instance_index])
# Calculate SHAP values for the single instance
shap_values_single = explainer.shap_values(single_instance)

# Extract the SHAP values for the instance
shap_values_instance = shap_values_single[0] if isinstance(shap_values_single, list) else shap_values_single

shap_value_plot(feature_names, shap_values_instance)

"""# **AGE-GENDER FILTERING**

JSON TO CSV CONVERSION (AGE-GENDER FILTERING)
"""

with open(ads_non_duplicate_path, 'r') as file:
    data = json.load(file)

processed_data = []

for item in data:

    selected_attributes = {
        'ad_creation_time': item.get('ad_creation_time'),
        'ad_creative_bodies': item.get('ad_creative_bodies'),
        'currency': item.get('currency'),
        'impressions': item.get('impressions'),
        'spend': item.get('spend'),
        'delivery_by_region': item.get('delivery_by_region', []),
        'demographic_distribution': item.get('demographic_distribution', [])
    }
    processed_data.append(selected_attributes)

df_age_filtered = pd.DataFrame(processed_data)

df_age_filtered.to_csv(ads_dataframe_age_filtered, index=False)

"""LOAD DATAFRAME (AGE-GENDER FILTERING) & GET INFORMATION"""

df_age_filtered = pd.read_csv(ads_dataframe_age_filtered)
print(df_age_filtered.dtypes)
df_age_filtered.head()

"""CONVERT INTO DATAFRAME (AGE-GENDER FILTERING)

"""

new_df_age_filtered = pd.DataFrame()

# Convert 'ad_creation_time' to datetime
new_df_age_filtered['ad_creation_time'] = pd.to_datetime(df_age_filtered['ad_creation_time'])

# Extract 'ad_creative_bodies'
new_df_age_filtered['ad_creative_bodies'] = df_age_filtered['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)

# Extract 'lower_bound' and 'upper_bound' values for impressions and spend
new_df_age_filtered['impressions_lower'] = df_age_filtered['impressions'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype("float")
new_df_age_filtered['impressions_upper'] = df_age_filtered['impressions'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype("float")
new_df_age_filtered['spend_lower'] = df_age_filtered['spend'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype("float")
new_df_age_filtered['spend_upper'] = df_age_filtered['spend'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype("float")

# Create columns for unique region names
'''
unique_regions = set()
for item in df['delivery_by_region']:
    try:
        regions = ast.literal_eval(item)
        for region in regions:
            if 'region' in region:
                unique_regions.add(region['region'])
    except (SyntaxError, ValueError):
        pass

for region in unique_regions:
    new_df_age_filtered[f'percentage_{region}'] = None
'''
# Create columns for age intervals
unique_age_intervals = set()
for item in df_age_filtered['demographic_distribution']:
    try:
        demographics = ast.literal_eval(item)
        for demographic in demographics:
            age = demographic.get('age', None)
            gender = demographic.get('gender', None)
            if age is not None and gender is not None:
                age_gender_key = f'{age}_{gender}'
                unique_age_intervals.add(age_gender_key)
    except (SyntaxError, ValueError):
        pass

for age_interval in unique_age_intervals:
    new_df_age_filtered[f'percentage_{age_interval}'] = None
'''
# Iterate through the data to fill in information or use percentages as empty
for i, item in enumerate(df['delivery_by_region']):
    try:
        regions = ast.literal_eval(item)
        for region in regions:
            if 'region' in region:
                new_df_age_filtered.at[i, f'percentage_{region["region"]}'] = region.get('percentage', None)
    except (SyntaxError, ValueError):
        pass
'''
for i, item in enumerate(df_age_filtered['demographic_distribution']):
    try:
        demographics = ast.literal_eval(item)
        for demographic in demographics:
            age = demographic.get('age', None)
            gender = demographic.get('gender', None)
            if age is not None and gender is not None:
                age_gender_key = f'{age}_{gender}'
                new_df_age_filtered.at[i, f'percentage_{age_gender_key}'] = demographic.get('percentage', None)
    except (SyntaxError, ValueError):
        pass

# Display data types and the head of the new DataFrame
print(new_df_age_filtered.dtypes)
new_df_age_filtered.head()

"""MERGE COLUMNS AND SORT ACCORDING TO DATE (AGE-GENDER FILTERING)"""

ads_dataframe_age_filtered = new_df_age_filtered.copy(deep=True)
ads_dataframe_age_filtered['cpi'] = (
    (new_df_age_filtered['spend_lower'].fillna(new_df_age_filtered['spend_upper']) / 2 +
     new_df_age_filtered['spend_upper'].fillna(new_df_age_filtered['spend_lower']) / 2) /
    (new_df_age_filtered['impressions_lower'].fillna(new_df_age_filtered['impressions_upper']) / 2 +
     new_df_age_filtered['impressions_upper'].fillna(new_df_age_filtered['impressions_lower']) / 2)
)
print(type(ads_dataframe_age_filtered))
ads_dataframe_age_filtered = ads_dataframe_age_filtered.drop(columns=['impressions_lower', 'impressions_upper', 'spend_lower', 'spend_upper'])
ads_dataframe_age_filtered = ads_dataframe_age_filtered.sort_values(by='ad_creation_time')

ads_dataframe_age_filtered

"""SAMPLE AGE-GENDER FILTERING DATASET SEPERATION"""

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

# Determine the split index
split_index = int(0.8 * ads_dataframe_age_filtered.shape[0])

train_data = ads_dataframe_age_filtered.iloc[:split_index]
test_data = ads_dataframe_age_filtered.iloc[split_index:]

gender_data_train = pd.DataFrame({
    'ad_creative_bodies': train_data['ad_creative_bodies'].astype(str),
    'percentage_25-34_male': train_data['percentage_25-34_male'].astype(str),
    'percentage_25-34_female': train_data['percentage_25-34_female'].astype(str),
    'percentage_35-44_male': train_data['percentage_35-44_male'].astype(str),
    'percentage_13-17_male': train_data['percentage_13-17_male'].astype(str),
    'percentage_55-64_male': train_data['percentage_55-64_male'].astype(str),
    'percentage_18-24_male': train_data['percentage_18-24_male'].astype(str),
    'percentage_65+_male': train_data['percentage_65+_male'].astype(str),
    'percentage_35-44_female': train_data['percentage_35-44_female'].astype(str),
    'percentage_13-17_female': train_data['percentage_13-17_female'].astype(str),
    'percentage_55-64_female': train_data['percentage_55-64_female'].astype(str),
    'percentage_18-24_female': train_data['percentage_18-24_female'].astype(str),
    'percentage_65+_female': train_data['percentage_65+_female'].astype(str),
    'percentage_45-54_male': train_data['percentage_45-54_male'].astype(str),
    'percentage_45-54_female': train_data['percentage_45-54_female'].astype(str),
    'cpi': train_data['cpi']
})

# Remove rows with 'None' values in the first two columns
gender_data_train.replace('None', np.nan, inplace=True)  # Replace 'None' with NaN
gender_data_train.dropna(subset=['percentage_25-34_male',
    'percentage_25-34_female',
    'percentage_35-44_male',
    'percentage_13-17_male',
    'percentage_55-64_male',
    'percentage_18-24_male',
    'percentage_65+_male',
    'percentage_35-44_female',
    'percentage_13-17_female',
    'percentage_55-64_female',
    'percentage_18-24_female',
    'percentage_65+_female',
    'percentage_45-54_male',
    'percentage_45-54_female'], how='any', inplace=True)

y_train_gendered = gender_data_train['cpi']
X_train_2 = gender_data_train['ad_creative_bodies']
gender_data_train.drop(columns=['cpi', 'ad_creative_bodies'], axis=1, inplace=True)

gender_data_test = pd.DataFrame({
    'ad_creative_bodies': test_data['ad_creative_bodies'].astype(str),
    'percentage_25-34_male': test_data['percentage_25-34_male'].astype(str),
    'percentage_25-34_female': test_data['percentage_25-34_female'].astype(str),
    'percentage_35-44_male': test_data['percentage_35-44_male'].astype(str),
    'percentage_13-17_male': test_data['percentage_13-17_male'].astype(str),
    'percentage_55-64_male': test_data['percentage_55-64_male'].astype(str),
    'percentage_18-24_male': test_data['percentage_18-24_male'].astype(str),
    'percentage_65+_male': test_data['percentage_65+_male'].astype(str),
    'percentage_35-44_female': test_data['percentage_35-44_female'].astype(str),
    'percentage_13-17_female': test_data['percentage_13-17_female'].astype(str),
    'percentage_55-64_female': test_data['percentage_55-64_female'].astype(str),
    'percentage_18-24_female': test_data['percentage_18-24_female'].astype(str),
    'percentage_65+_female': test_data['percentage_65+_female'].astype(str),
    'percentage_45-54_male': test_data['percentage_45-54_male'].astype(str),
    'percentage_45-54_female': test_data['percentage_45-54_female'].astype(str),
    'cpi': test_data['cpi']
})

# Remove rows with 'None' values in the first two columns
gender_data_test.replace('None', np.nan, inplace=True)  # Replace 'None' with NaN
gender_data_test.dropna(subset=['percentage_25-34_male',
    'percentage_25-34_female',
    'percentage_35-44_male',
    'percentage_13-17_male',
    'percentage_55-64_male',
    'percentage_18-24_male',
    'percentage_65+_male',
    'percentage_35-44_female',
    'percentage_13-17_female',
    'percentage_55-64_female',
    'percentage_18-24_female',
    'percentage_65+_female',
    'percentage_45-54_male',
    'percentage_45-54_female'], how='any', inplace=True)

y_test_gendered = gender_data_test['cpi']
X_test_2 = gender_data_test['ad_creative_bodies']
gender_data_test.drop('cpi', axis=1, inplace=True)

# Tokenization and stopword removal
tokenized_text_train_2 = X_train_2.apply(tokenize_and_remove_stopwords)
tokenized_text_test_2 = X_test_2.apply(tokenize_and_remove_stopwords)

# Word2Vec model training
word2vec_model = Word2Vec(sentences=tokenized_text_train_2, vector_size=100, window=5, min_count=1, workers=4)

# Generate document vectors
X_train_word2vec_2 = generate_doc_vectors(tokenized_text_train_2, word2vec_model, 100)
X_test_word2vec_2 = generate_doc_vectors(tokenized_text_test_2, word2vec_model, 100)

#word2vec_model.save("word2vec_model.model")

gender_data_train.reset_index(drop=True, inplace=True)
gender_data_test.reset_index(drop=True, inplace=True)

combined_train_data = pd.concat([gender_data_train, pd.DataFrame(X_train_word2vec_2)], axis=1)
combined_test_data = pd.concat([gender_data_test, pd.DataFrame(X_test_word2vec_2)], axis=1)

combined_train_data.columns = combined_train_data.columns.astype(str)
combined_test_data.columns = combined_test_data.columns.astype(str)
combined_test_data.drop(columns=['ad_creative_bodies'], inplace=True)##Comment when not necessary

# Initialize the Gradient Boosting Regressor
gb_model_gender = GradientBoostingRegressor(n_estimators=200, learning_rate=0.001, random_state=42, loss="absolute_error")

# Fit the model
gb_model_gender.fit(combined_train_data, y_train_gendered)

"""SAVE GENDERED MODEL"""

joblib.dump(gb_model_gender, '/content/drive/MyDrive/CS491MLMODEL/us/us/gb_model_gender.joblib')

"""SHAP VALUE FOR GENDER COLUMNS"""

print(feature_names)

feature_names = combined_train_data.columns.tolist()

explainer = shap.TreeExplainer(gb_model_gender)

instance_index = 157  # Index of the instance in the test set

single_instance = combined_test_data[instance_index:]

# Calculate SHAP values for the single instance
shap_values_single = explainer.shap_values(single_instance)

# Extract the SHAP values for the instance
shap_values_instance = shap_values_single[0] if isinstance(shap_values_single, list) else shap_values_single

shap_value_plot(feature_names[0:14], shap_values_instance[:,:14])

"""# **IMPRESSION PREDICTION WITH BERT**

CLEANING FOR BERT
"""

import re

def filter_ibans(text):
    pattern = r'fr\d{2}[ ]\d{4}[ ]\d{4}[ ]\d{4}[ ]\d{4}[ ]\d{2}|fr\d{20}|fr[ ]\d{2}[ ]\d{3}[ ]\d{3}[ ]\d{3}[ ]\d{5}'
    text = re.sub(pattern, '', text)
    return text

def remove_space_between_numbers(text):
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    return text

def filter_emails(text):
    pattern = r'(?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\"[a-zA-Z0-9.+!% -]{1,64}\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,})'
    text = re.sub(pattern, '', text)
    return text

def filter_ref(text):
    pattern = r'(\(*)(ref|réf)(\.|[ ])\d+(\)*)'
    text = re.sub(pattern, '', text)

    return text
def filter_websites(text):
    pattern = r'(http\:\/\/|https\:\/\/)?([a-z0-9][a-z0-9\-]*\.)+[a-z][a-z\-]*'
    text = re.sub(pattern, '', text)
    return text

def filter_phone_numbers(text):
    pattern = r'(?:(?:\+|00)33[\s.-]{0,3}(?:\(0\)[\s.-]{0,3})?|0)[1-9](?:(?:[\s.-]?\d{2}){4}|\d{2}(?:[\s.-]?\d{3}){2})|(\d{2}[ ]\d{2}[ ]\d{3}[ ]\d{3})'
    text = re.sub(pattern, '', text)
    return text

def clean_text(text):
    text = text.lower()
    text = text.replace(u'\xa0', u' ')
    text = filter_phone_numbers(text)
    text = filter_emails(text)
    text = filter_ibans(text)
    text = filter_ref(text)
    text = filter_websites(text)
    text = remove_space_between_numbers(text)
    return text

train_data['ad_creative_bodies'] = train_data.ad_creative_bodies.apply(clean_text)

from transformers import DistilBertTokenizer

# Specify the model name
model_name = "distilbert-base-uncased"

# Load the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

encoded_corpus = tokenizer(text=train_data.ad_creative_bodies.tolist(),
                            add_special_tokens=True,
                            padding='max_length',
                            truncation='longest_first',
                            max_length=300,
                            return_attention_mask=True)

input_ids = encoded_corpus['input_ids']
attention_mask = encoded_corpus['attention_mask']

import numpy as np

def filter_long_descriptions(tokenizer, descriptions, max_len):
    indices = []
    lengths = tokenizer(descriptions, padding=False,
                     truncation=False, return_length=True)['length']
    for i in range(len(descriptions)):
        if lengths[i] <= max_len-2:
            indices.append(i)
    return indices

short_descriptions = filter_long_descriptions(tokenizer,
                               train_data.ad_creative_bodies.tolist(), 300)

input_ids = np.array(input_ids)[short_descriptions]
attention_mask = np.array(attention_mask)[short_descriptions]
labels = train_data.impressions.to_numpy()[short_descriptions].astype(np.float32) #target'ı impressions yaptım şu anlık
spends = train_data.spend.to_numpy()[short_descriptions].astype(np.float32) #cost'u impressions yaptım şu anlık

from sklearn.model_selection import train_test_split
validation_size = 0.1
seed = 42
train_inputs, validation_inputs, train_labels, validation_labels = \
            train_test_split(input_ids, labels, test_size=validation_size,
                             random_state=seed)

train_masks, validation_masks, _, _ = train_test_split(attention_mask,
                                        labels, test_size=validation_size,
                                        random_state=seed)

train_spends, validation_spends, _, _ = train_test_split(spends,
                                        labels, test_size=validation_size,
                                        random_state=seed)

from sklearn.preprocessing import StandardScaler
impression_scaler = StandardScaler()
impression_scaler.fit(train_labels.reshape(-1, 1))
train_labels = impression_scaler.transform(train_labels.reshape(-1, 1))
validation_labels = impression_scaler.transform(validation_labels.reshape(-1, 1))

cost_scaler = StandardScaler()
cost_scaler.fit(train_spends.reshape(-1, 1))
train_spends = cost_scaler.transform(train_spends.reshape(-1, 1))
validation_spends = cost_scaler.transform(validation_spends.reshape(-1, 1))

import torch
from torch.utils.data import TensorDataset, DataLoader
batch_size = 64
def create_dataloaders(inputs, masks, labels, spends, batch_size):
    input_tensor = torch.tensor(inputs)
    mask_tensor = torch.tensor(masks)
    labels_tensor = torch.tensor(labels)
    spends_tensor = torch.tensor(spends)
    dataset = TensorDataset(input_tensor, mask_tensor,
                            labels_tensor, spends_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size,
                            shuffle=True, pin_memory=True)
    return dataloader
train_dataloader = create_dataloaders(train_inputs, train_masks,
                                      train_labels, train_spends, batch_size)
validation_dataloader = create_dataloaders(validation_inputs, validation_masks,
                                     validation_labels, validation_spends, batch_size)

import torch.nn as nn
from transformers import DistilBertModel
class DistilBertRegressor(nn.Module):

    def __init__(self, drop_rate=0.2, freeze_bert=False):

        super(DistilBertRegressor, self).__init__()
        D_in, D_out = 768 + 1, 1

        self.distilbert = \
                   DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.regressor = nn.Sequential(
            nn.Dropout(drop_rate),
              nn.Linear(D_in, D_out))

    def forward(self, input_ids, attention_masks, spends):
        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_masks)
        hidden_state = outputs[0]  # (batch_size, seq_length, hidden_size)
        pooled_output = hidden_state[:, 0]  # Use the first token's embeddings
        # Concatenate pooled_output with spends
        combined_input = torch.cat((pooled_output, spends), dim=1)

        outputs = self.regressor(combined_input)
        return outputs


model = DistilBertRegressor(drop_rate=0.2)
model = model.to(torch.float)

import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using GPU.")
else:
    print("No GPU available, using the CPU instead.")
    device = torch.device("cpu")
model.to(device)

from transformers import AdamW
optimizer = AdamW(model.parameters(),
                  lr=5e-5,
                  eps=1e-8)

from transformers import get_linear_schedule_with_warmup
epochs = 5
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,
                 num_warmup_steps=0, num_training_steps=total_steps)

loss_function = nn.MSELoss()

import torch
from torch.nn.utils.clip_grad import clip_grad_norm_
from sklearn.metrics import r2_score

def train(model, optimizer, scheduler, loss_function, epochs,
          train_dataloader, device, clip_value=2):
    loss_list = []  # List to store loss values
    r2_list = []  # List to store R2 scores
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        print("-----")
        model.train()
        for step, batch in enumerate(train_dataloader):
            batch_inputs, batch_masks, batch_labels, batch_spends = tuple(b.to(device) for b in batch)
            model.zero_grad()
            outputs = model(batch_inputs, batch_masks, batch_spends)
            loss = loss_function(outputs.squeeze(), batch_labels.squeeze())
            loss.backward()
            clip_grad_norm_(model.parameters(), clip_value)
            optimizer.step()
            scheduler.step()

            # Calculate and save metrics every 20 batches
            if step % 100 == 0:
                # Calculate R2 score
                outputs_np = outputs.squeeze().detach().cpu().numpy()
                labels_np = batch_labels.squeeze().detach().cpu().numpy()
                r2 = r2_score(labels_np, outputs_np)
                r2_list.append((epoch, step, r2))
                loss_list.append((epoch, step, loss.item()))
                print(f"Batch {step}: Loss = {loss.item()}, R2 = {r2}")

    return model, loss_list, r2_list

# Sample call to the modified train function
model, loss_list, r2_list = train(model, optimizer, scheduler, loss_function, epochs,
                                  train_dataloader, device, clip_value=2)

"""SAVE THE MODEL"""

torch.save(model.state_dict(), "/content/drive/MyDrive/CS491MLMODEL/us/us/bert_regression.pth")

"""EVALUATION SET"""

def evaluate(model, loss_function, test_dataloader, device):
    model.eval()
    test_loss, test_r2 = [], []
    for batch in test_dataloader:
        batch_inputs, batch_masks, batch_labels = \
                                 tuple(b.to(device) for b in batch)
        with torch.no_grad():
            outputs = model(batch_inputs, batch_masks)
        loss = loss_function(outputs, batch_labels)
        test_loss.append(loss.item())
        r2 = r2_score(outputs, batch_labels)
        test_r2.append(r2.item())
    return test_loss, test_r2

def r2_score(outputs, labels):
    labels_mean = torch.mean(labels)
    ss_tot = torch.sum((labels - labels_mean) ** 2)
    ss_res = torch.sum((labels - outputs) ** 2)
    r2 = 1 - ss_res / ss_tot
    return r2

"""PREDICTION"""

def predict(model, dataloader, device):
    model.eval()
    output = []
    for batch in dataloader:
        batch_inputs, batch_masks, _ = \
                                  tuple(b.to(device) for b in batch)
        with torch.no_grad():
            output += model(batch_inputs,
                            batch_masks).view(1,-1).tolist()[0]
    return output

val_set = val_data[['id_annonce', 'description', 'prix']]
val_set['cleaned_description'] = \
                val_set.description.apply(clean_text)
encoded_val_corpus = \
                tokenizer(text=val_set.cleaned_description.tolist(),
                          add_special_tokens=True,
                          padding='max_length',
                          truncation='longest_first',
                          max_length=300,
                          return_attention_mask=True)
val_input_ids = np.array(encoded_val_corpus['input_ids'])
val_attention_mask = np.array(encoded_val_corpus['attention_mask'])
val_labels = val_set.prix.to_numpy()
val_labels = price_scaler.transform(val_labels.reshape(-1, 1))
val_dataloader = create_dataloaders(val_input_ids,
                         val_attention_mask, val_labels, batch_size)
y_pred_scaled = predict(model, val_dataloader, device)

y_test = val_set.prix.to_numpy()
y_pred = price_scaler.inverse_transform(y_pred_scaled)

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score
mae = mean_absolute_error(y_test, y_pred)
mdae = median_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
mdape = ((pd.Series(y_test) - pd.Series(y_pred))\
         / pd.Series(y_test)).abs().median()
r_squared = r2_score(y_test, y_pred)

"""# **AGE PROBABILITY CLASSIFICATION WITH BERT**

LOAD DATAFRAME
"""

df_age_filtered = pd.read_csv(ads_dataframe_age_filtered)
print(df_age_filtered.dtypes)
df_age_filtered.head()

"""CONVERT INTO DATAFRAME (AGE-GENDER FILTERING)"""

new_df_age_filtered = pd.DataFrame()

# Convert 'ad_creation_time' to datetime
new_df_age_filtered['ad_creation_time'] = pd.to_datetime(df_age_filtered['ad_creation_time'])

# Extract 'ad_creative_bodies'
new_df_age_filtered['ad_creative_bodies'] = df_age_filtered['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)

# Create columns for age intervals
unique_age_intervals = set()
for item in df_age_filtered['demographic_distribution']:
    try:
        demographics = ast.literal_eval(item)
        for demographic in demographics:
            age = demographic.get('age', None)
            gender = demographic.get('gender', None)
            if age is not None and gender is not None:
                age_gender_key = f'{age}_{gender}'
                unique_age_intervals.add(age_gender_key)
    except (SyntaxError, ValueError):
        pass

for age_interval in unique_age_intervals:
    new_df_age_filtered[f'percentage_{age_interval}'] = pd.Series(dtype='float32')

for i, item in enumerate(df_age_filtered['demographic_distribution']):
    try:
        demographics = ast.literal_eval(item)
        for demographic in demographics:
            age = demographic.get('age', None)
            gender = demographic.get('gender', None)
            if age is not None and gender is not None:
                age_gender_key = f'{age}_{gender}'
                new_df_age_filtered.at[i, f'percentage_{age_gender_key}'] = np.float32(demographic.get('percentage', 0))
    except (SyntaxError, ValueError):
        pass

# Display data types and the head of the new DataFrame
print(new_df_age_filtered.dtypes)
new_df_age_filtered.fillna(0, inplace=True)
new_df_age_filtered.head()

"""MERGE COLUMNS AND SORT ACCORDING TO DATE"""

ads_dataframe_age_filtered = pd.DataFrame()
ads_dataframe_age_filtered['ad_creation_time'] = new_df_age_filtered['ad_creation_time']
ads_dataframe_age_filtered['ad_creative_bodies'] = new_df_age_filtered['ad_creative_bodies']

unknown_column = new_df_age_filtered['percentage_Unknown_unknown'] / 7

ads_dataframe_age_filtered['age-13-17'] = new_df_age_filtered["percentage_13-17_male"] + new_df_age_filtered["percentage_13-17_female"] + new_df_age_filtered["percentage_13-17_unknown"] + unknown_column
ads_dataframe_age_filtered['age-18-24'] = new_df_age_filtered["percentage_18-24_male"] + new_df_age_filtered["percentage_18-24_female"] + new_df_age_filtered["percentage_18-24_unknown"] + unknown_column
ads_dataframe_age_filtered['age-25-34'] = new_df_age_filtered["percentage_25-34_male"] + new_df_age_filtered["percentage_25-34_female"] + new_df_age_filtered["percentage_25-34_unknown"] + unknown_column
ads_dataframe_age_filtered['age-35-44'] = new_df_age_filtered["percentage_35-44_male"] + new_df_age_filtered["percentage_35-44_female"] + new_df_age_filtered["percentage_35-44_unknown"] + unknown_column
ads_dataframe_age_filtered['age-45-54'] = new_df_age_filtered["percentage_45-54_male"] + new_df_age_filtered["percentage_45-54_female"] + new_df_age_filtered["percentage_45-54_unknown"] + unknown_column
ads_dataframe_age_filtered['age-55-64'] = new_df_age_filtered["percentage_55-64_male"] + new_df_age_filtered["percentage_55-64_female"] + new_df_age_filtered["percentage_55-64_unknown"] + unknown_column
ads_dataframe_age_filtered['age-65+'] = new_df_age_filtered["percentage_65+_male"] + new_df_age_filtered["percentage_65+_female"] + new_df_age_filtered["percentage_65+_unknown"] + unknown_column

ads_dataframe_age_filtered

"""SPLIT TRAIN & TEST"""

# Determine the split index
split_value = 0.1
split_index = int(split_value * len(ads_dataframe_age_filtered))

train_data_age = ads_dataframe_age_filtered.iloc[:split_index]
test_data_age = ads_dataframe_age_filtered.iloc[split_index:]

train_data_age['ad_creative_bodies'] = train_data_age.ad_creative_bodies.apply(clean_text)

from transformers import DistilBertTokenizer

# Specify the model name
model_name = "distilbert-base-uncased"

# Load the tokenizer
tokenizer_age = DistilBertTokenizer.from_pretrained(model_name)

encoded_corpus_age = tokenizer_age(text=train_data_age.ad_creative_bodies.tolist(),
                            add_special_tokens=True,
                            padding='max_length',
                            truncation='longest_first',
                            max_length=300,
                            return_attention_mask=True)

input_ids_age = encoded_corpus_age['input_ids']
attention_mask_age = encoded_corpus_age['attention_mask']

import numpy as np

short_descriptions_age = filter_long_descriptions(tokenizer_age,
                               train_data_age.ad_creative_bodies.tolist(), 300)

input_ids_age = np.array(input_ids_age)[short_descriptions_age]
attention_mask_age = np.array(attention_mask_age)[short_descriptions_age]
# Select the age columns as labels. Adjust the column names as needed.
age_columns = ['age-13-17', 'age-18-24', 'age-25-34', 'age-35-44', 'age-45-54', 'age-55-64', 'age-65+']
labels_age = train_data_age[age_columns].to_numpy()[short_descriptions_age].astype(np.float32)

from sklearn.model_selection import train_test_split
validation_size = 0.1
seed = 42
train_inputs_age, validation_inputs_age, train_labels_age, validation_labels_age = \
            train_test_split(input_ids_age, labels_age, test_size=validation_size,
                             random_state=seed)

train_masks_age, validation_masks_age, _, _ = train_test_split(attention_mask_age,
                                        labels_age, test_size=validation_size,
                                        random_state=seed)

import torch
from torch.utils.data import TensorDataset, DataLoader
batch_size = 64
def create_dataloaders(inputs, masks, labels, batch_size):
    input_tensor = torch.tensor(inputs)
    mask_tensor = torch.tensor(masks)
    labels_tensor = torch.tensor(labels)
    dataset = TensorDataset(input_tensor, mask_tensor,
                            labels_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size,
                            shuffle=True, pin_memory=True)
    return dataloader
train_dataloader_age = create_dataloaders(train_inputs_age, train_masks_age,
                                      train_labels_age, batch_size)
validation_dataloader_age = create_dataloaders(validation_inputs_age, validation_masks_age,
                                     validation_labels_age, batch_size)

import torch
import torch.nn as nn
from transformers import DistilBertModel

class DistilBertClassifierAge(nn.Module):

    def __init__(self, drop_rate=0.2):
        super(DistilBertClassifierAge, self).__init__()
        # Input dimension for each token embedding from BERT
        D_in, D_out = 768, 7  # Output dimension matches the number of age categories

        self.distilbert = DistilBertModel.from_pretrained("distilbert-base-uncased")
        # Define the regressor with a Dropout and a Linear layer
        self.regressor = nn.Sequential(
            nn.Dropout(drop_rate),
            nn.Linear(D_in, D_out),
            nn.Softmax(dim=1)  # Ensure outputs are probabilities that sum to 1
        )

    def forward(self, input_ids, attention_masks):
        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_masks)
        hidden_state = outputs[0]  # (batch_size, seq_length, hidden_size)
        pooled_output = hidden_state[:, 0]  # Use the first token's embeddings (CLS token)

        # Pass pooled_output through regressor to get final output
        logits = self.regressor(pooled_output)
        return logits

# Example of model creation and moving model to a device
model_age = DistilBertClassifierAge(drop_rate=0.2)
model_age = model_age.to(torch.float)

import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using GPU.")
else:
    print("No GPU available, using the CPU instead.")
    device = torch.device("cpu")
model_age.to(device)

from transformers import AdamW
optimizer_age = AdamW(model_age.parameters(),
                  lr=5e-5,
                  eps=1e-8)

from transformers import get_linear_schedule_with_warmup
epochs_age = 5
total_steps_age = len(train_dataloader_age) * epochs_age
scheduler_age = get_linear_schedule_with_warmup(optimizer_age,
                 num_warmup_steps=0, num_training_steps=total_steps_age)

loss_function_age = nn.KLDivLoss(reduction='batchmean')

import torch
from torch.nn.utils.clip_grad import clip_grad_norm_
from sklearn.metrics import accuracy_score

def train_age(model, optimizer, scheduler, loss_function, epochs,
          train_dataloader, device, clip_value=2):
    loss_list = []  # List to store loss values
    accuracy_list = []  # List to store accuracy scores
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        print("-----")
        model.train()
        total_correct = 0
        total_samples = 0
        for step, batch in enumerate(train_dataloader):
            batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)
            model.zero_grad()
            outputs = model(batch_inputs, batch_masks)
            loss = loss_function(outputs, batch_labels)
            loss.backward()
            clip_grad_norm_(model.parameters(), clip_value)
            optimizer.step()
            scheduler.step()

            # Calculate and save metrics every 100 batches
            if step % 50 == 0:
                # Calculate accuracy
                _, predicted = torch.max(outputs, 1)
                #total_correct = (predicted == labels_indices).sum().item()
                #total_samples = labels_indices.size(0)
                #accuracy = total_correct / total_samples
                #accuracy_list.append((epoch, step, accuracy))
                loss_list.append((epoch, step, loss.item()))
                print(f"Batch {step}: Loss = {loss.item()}")

    return model, loss_list

# Sample call to the modified train function
model_age, loss_list_age = train_age(model_age, optimizer_age, scheduler_age, loss_function_age, epochs_age,
                                  train_dataloader_age, device, clip_value=2)