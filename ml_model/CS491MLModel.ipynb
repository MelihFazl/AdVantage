{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INSTALLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ijson\n",
            "  Downloading ijson-3.2.3-cp311-cp311-win_amd64.whl (48 kB)\n",
            "                                              0.0/48.2 kB ? eta -:--:--\n",
            "     -----------------                        20.5/48.2 kB ? eta -:--:--\n",
            "     ------------------------               30.7/48.2 kB 660.6 kB/s eta 0:00:01\n",
            "     -------------------------------------- 48.2/48.2 kB 486.3 kB/s eta 0:00:00\n",
            "Installing collected packages: ijson\n",
            "Successfully installed ijson-3.2.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
            "[notice] To update, run: C:\\Users\\Trelans\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install ijson\n",
        "!pip install shap\n",
        "!pip install pdpbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "JSON TO CSV CONVERSION AGE FILTERING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/Bitirme/ads_non_duplicate.json'\n",
        "ads_dataframe = '/content/drive/MyDrive/Bitirme/ads_dataframe.csv'\n",
        "\n",
        "with open(ads_non_duplicate_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "for item in data:\n",
        "   \n",
        "    selected_attributes = {\n",
        "        'ad_creation_time': item.get('ad_creation_time'),\n",
        "        'ad_creative_bodies': item.get('ad_creative_bodies'),\n",
        "        'currency': item.get('currency'),\n",
        "        'impressions': item.get('impressions'),\n",
        "        'spend': item.get('spend'),\n",
        "        'delivery_by_region': item.get('delivery_by_region', []),\n",
        "        'demographic_distribution': item.get('demographic_distribution', [])\n",
        "    }\n",
        "    processed_data.append(selected_attributes)\n",
        "\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "df.to_csv(ads_dataframe, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9E0Lj_tGzD1"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfuYr5k5G1QY",
        "outputId": "694de50a-031e-43a0-a719-d48ebb257011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import ijson\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import shap\n",
        "from pdpbox import pdp\n",
        "import joblib\n",
        "import json\n",
        "import gzip\n",
        "import pdpbox\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import nltk\n",
        "import ast\n",
        "from nltk.corpus import stopwords\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "ads_dataset_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads.json'\n",
        "ads_downsized_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_downsized.json'\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'\n",
        "ads_dataframe = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_dataframe.csv'\n",
        "ads_non_duplicate_path = '/content/drive/MyDrive/CS491MLMODEL/us/us/ads_non_duplicate.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERT INTO DATAFRAME AGE FILTERING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Assuming 'df' is your original DataFrame\n",
        "\n",
        "# Create a new DataFrame\n",
        "new_df = pd.DataFrame()\n",
        "\n",
        "# Convert 'ad_creation_time' to datetime\n",
        "new_df['ad_creation_time'] = pd.to_datetime(df['ad_creation_time'])\n",
        "\n",
        "# Extract 'ad_creative_bodies'\n",
        "new_df['ad_creative_bodies'] = df['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)\n",
        "\n",
        "# Extract 'lower_bound' and 'upper_bound' values for impressions and spend\n",
        "new_df['impressions_lower'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['impressions_upper'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "new_df['spend_lower'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['spend_upper'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "# Create columns for unique region names\n",
        "'''\n",
        "unique_regions = set()\n",
        "for item in df['delivery_by_region']:\n",
        "    try:\n",
        "        regions = ast.literal_eval(item)\n",
        "        for region in regions:\n",
        "            if 'region' in region:\n",
        "                unique_regions.add(region['region'])\n",
        "    except (SyntaxError, ValueError):\n",
        "        pass\n",
        "\n",
        "for region in unique_regions:\n",
        "    new_df[f'percentage_{region}'] = None\n",
        "'''\n",
        "# Create columns for age intervals\n",
        "unique_age_intervals = set()\n",
        "for item in df['demographic_distribution']:\n",
        "    try:\n",
        "        demographics = ast.literal_eval(item)\n",
        "        for demographic in demographics:\n",
        "            age = demographic.get('age', None)\n",
        "            gender = demographic.get('gender', None)\n",
        "            if age is not None and gender is not None:\n",
        "                age_gender_key = f'{age}_{gender}'\n",
        "                unique_age_intervals.add(age_gender_key)\n",
        "    except (SyntaxError, ValueError):\n",
        "        pass\n",
        "\n",
        "for age_interval in unique_age_intervals:\n",
        "    new_df[f'percentage_{age_interval}'] = None\n",
        "'''\n",
        "# Iterate through the data to fill in information or use percentages as empty\n",
        "for i, item in enumerate(df['delivery_by_region']):\n",
        "    try:\n",
        "        regions = ast.literal_eval(item)\n",
        "        for region in regions:\n",
        "            if 'region' in region:\n",
        "                new_df.at[i, f'percentage_{region[\"region\"]}'] = region.get('percentage', None)\n",
        "    except (SyntaxError, ValueError):\n",
        "        pass\n",
        "'''\n",
        "for i, item in enumerate(df['demographic_distribution']):\n",
        "    try:\n",
        "        demographics = ast.literal_eval(item)\n",
        "        for demographic in demographics:\n",
        "            age = demographic.get('age', None)\n",
        "            gender = demographic.get('gender', None)\n",
        "            if age is not None and gender is not None:\n",
        "                age_gender_key = f'{age}_{gender}'\n",
        "                new_df.at[i, f'percentage_{age_gender_key}'] = demographic.get('percentage', None)\n",
        "    except (SyntaxError, ValueError):\n",
        "        pass\n",
        "\n",
        "# Display data types and the head of the new DataFrame\n",
        "print(new_df.dtypes)\n",
        "print(new_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGrHo_MILle"
      },
      "source": [
        "DATA DOWNSIZING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59BCGxSBINGp",
        "outputId": "1f67efcc-d839-4e67-ff13-9c88e0bd7d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "767264 items [05:44, 2227.86 items/s]                        \n"
          ]
        }
      ],
      "source": [
        "json_item_heuristic = 3000000\n",
        "\n",
        "def downsize_data(num_of_items, input_dataset_path, output_dataset_path):\n",
        "    counter = 0\n",
        "    with open(input_dataset_path, 'r') as input_file:\n",
        "        parser = ijson.items(input_file, 'item')\n",
        "\n",
        "        with open(output_dataset_path, 'a') as outfile:  # Open output file once\n",
        "            outfile.write('[\\n')\n",
        "            # Use tqdm to create a progress bar\n",
        "            for item in tqdm(parser, total=int(num_of_items), unit=\" items\"):\n",
        "                if counter > num_of_items:\n",
        "                    break\n",
        "                counter += 1\n",
        "                json.dump(item, outfile)\n",
        "                if counter > num_of_items:\n",
        "                  outfile.write('\\n')  # Add a newline between items\n",
        "                else:\n",
        "                  outfile.write(',\\n')  # Add a newline between items\n",
        "\n",
        "            outfile.write(']\\n')\n",
        "            outfile.close()\n",
        "\n",
        "downsize_data(json_item_heuristic, ads_dataset_path, ads_downsized_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA DUPLICATE REMOVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_duplicates(filename, column, output_filename):\n",
        "    json_item_heuristic = 3000\n",
        "    first_item = True\n",
        "    removed_content = 0\n",
        "    unique_values = set()\n",
        "\n",
        "    with open(filename, 'rb') as file:\n",
        "        items = ijson.items(file, 'item')\n",
        "\n",
        "        with open(output_filename, 'w') as outfile:\n",
        "            outfile.write('[\\n')\n",
        "            for item in tqdm(items, total=int(json_item_heuristic), unit=\" items\"):\n",
        "                if 'ad_creative_bodies' in item:\n",
        "                    column_value = tuple(item['ad_creative_bodies'])\n",
        "\n",
        "                    if (column_value not in unique_values) and (len(column_value) == 1):\n",
        "                        unique_values.add(column_value)\n",
        "                        if first_item:\n",
        "                            json.dump(item, outfile)\n",
        "                            first_item = False\n",
        "                        else:\n",
        "                            outfile.write(',\\n')\n",
        "                            json.dump(item, outfile)\n",
        "                    else:\n",
        "                        removed_content += 1\n",
        "                else:\n",
        "                    removed_content += 1\n",
        "                    pass\n",
        "            outfile.write(']\\n')\n",
        "    print(f\"Amount of ads that have been filtered: {removed_content}\")\n",
        "\n",
        "\n",
        "remove_duplicates(ads_downsized_path, 'a', ads_non_duplicate_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "JSON TO CSV CONVERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(ads_non_duplicate_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "processed_data = []\n",
        "for item in data:\n",
        "    selected_attributes = {'ad_creation_time': item['ad_creation_time'],\n",
        "                           'ad_creative_bodies': item['ad_creative_bodies'],\n",
        "                           'currency': item['currency'],\n",
        "                           'impressions': item['impressions'],\n",
        "                           'spend': item['spend']}\n",
        "    processed_data.append(selected_attributes)\n",
        "\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "df.to_csv(ads_dataframe, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOAD DATAFRAME & GET INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(ads_dataframe)\n",
        "print(df.dtypes)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERT INTO DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = pd.DataFrame()\n",
        "\n",
        "new_df['ad_creation_time'] = pd.to_datetime(df['ad_creation_time'])\n",
        "new_df['ad_creative_bodies'] = df['ad_creative_bodies'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else x).astype(str)\n",
        "\n",
        "\n",
        "# Extract 'lower_bound' and 'upper_bound' values if they exist, otherwise use NaN\n",
        "new_df['impressions_lower'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['impressions_upper'] = df['impressions'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "new_df['spend_lower'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('lower_bound', None)).astype(\"float\")\n",
        "new_df['spend_upper'] = df['spend'].apply(lambda x: ast.literal_eval(x).get('upper_bound', None)).astype(\"float\")\n",
        "\n",
        "print(new_df.dtypes)\n",
        "\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MERGE COLUMNS AND SORT ACCORDING TO DATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ads_dataframe = pd.DataFrame()\n",
        "ads_dataframe['ad_creation_time'] = new_df['ad_creation_time']\n",
        "ads_dataframe['ad_creative_bodies'] = new_df['ad_creative_bodies']\n",
        "ads_dataframe['cpi'] = (\n",
        "    (new_df['spend_lower'].fillna(new_df['spend_upper']) / 2 +\n",
        "     new_df['spend_upper'].fillna(new_df['spend_lower']) / 2) /\n",
        "    (new_df['impressions_lower'].fillna(new_df['impressions_upper']) / 2 +\n",
        "     new_df['impressions_upper'].fillna(new_df['impressions_lower']) / 2)\n",
        ")\n",
        "\n",
        "ads_dataframe = ads_dataframe.sort_values(by='ad_creation_time')\n",
        "ads_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VECTORIZE TEXTUAL CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "def generate_doc_vectors(docs, model, num_features):\n",
        "    doc_vectors = [average_word_vectors(doc, model, model.wv.index_to_key, num_features) for doc in docs]\n",
        "    return np.array(doc_vectors)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Determine the split index\n",
        "split_index = int(0.8 * len(ads_dataframe))\n",
        "\n",
        "train_data = ads_dataframe.iloc[:split_index]\n",
        "test_data = ads_dataframe.iloc[split_index:]\n",
        "\n",
        "X_train = train_data['ad_creative_bodies'].astype(str)\n",
        "y_train = train_data['cpi']\n",
        "\n",
        "X_test = test_data['ad_creative_bodies'].astype(str)\n",
        "y_test = test_data['cpi']\n",
        "\n",
        "# Tokenization and stopword removal\n",
        "tokenized_text_train = X_train.apply(tokenize_and_remove_stopwords)\n",
        "tokenized_text_test = X_test.apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Word2Vec model training\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text_train, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate document vectors\n",
        "X_train_word2vec = generate_doc_vectors(tokenized_text_train, word2vec_model, 100)\n",
        "X_test_word2vec = generate_doc_vectors(tokenized_text_test, word2vec_model, 100)\n",
        "\n",
        "#word2vec_model.save(\"word2vec_model.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAIN ML MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model training and evaluation\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train_word2vec, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(gb_regressor, '/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')\n",
        "#loaded_model = joblib.load('/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model = joblib.load('/content/drive/MyDrive/CS491MLMODEL/us/us/gradient_boosting_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TEST WITH NDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create subsets of data\n",
        "def create_data_subsets(data, group_size):\n",
        "    grouped_data = [data.iloc[i : i + group_size] for i in range(0 , data.shape[0], group_size)]\n",
        "    return grouped_data\n",
        "\n",
        "def calculate_ndcg_for_groups(groups, model):\n",
        "    ndcg_scores = []\n",
        "    for group in tqdm(groups, total=len(groups), unit=\" items\"):\n",
        "        tokenized_X_train_group = group['ad_creative_bodies'].apply(tokenize_and_remove_stopwords)\n",
        "        X_train_word2vec_group = generate_doc_vectors(tokenized_X_train_group, word2vec_model, 100)\n",
        "\n",
        "        y_group_pred = model.predict(X_train_word2vec_group)\n",
        "\n",
        "        # Assuming lower CPI values are more relevant\n",
        "        relevance_scores = 1 / y_group_pred\n",
        "\n",
        "        # Normalize relevance scores (optional)\n",
        "        #relevance_scores = (relevance_scores - np.min(relevance_scores)) / (np.max(relevance_scores) - np.min(relevance_scores))\n",
        "\n",
        "        y_group_true = 1 / group['cpi'].values\n",
        "\n",
        "        try:\n",
        "            ndcg = ndcg_score([y_group_true], [relevance_scores])\n",
        "            ndcg_scores.append(ndcg)\n",
        "        except Exception as e:\n",
        "            print(y_group_true)\n",
        "            print(relevance_scores)\n",
        "\n",
        "    return ndcg_scores\n",
        "\n",
        "group_size = 4\n",
        "test_data_subsets = create_data_subsets(test_data, group_size)\n",
        "\n",
        "ndcg_scores = calculate_ndcg_for_groups(test_data_subsets, gb_regressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PRINT NDCG SCORES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine NDCG scores across groups (simple average in this example)\n",
        "average_ndcg_across_groups = sum(ndcg_scores) / len(ndcg_scores)\n",
        "\n",
        "print(f\"Average NDCG Across Groups: {round(average_ndcg_across_groups,2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RECOMMENDATION MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the split index\n",
        "split_index = int(0.8 * len(ads_dataframe))\n",
        "\n",
        "train_data = ads_dataframe.iloc[:split_index]\n",
        "test_data = ads_dataframe.iloc[split_index:]\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "max_features = 50  # Choose the top most important words\n",
        "vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
        "X_train_suggestion = vectorizer.fit_transform(train_data['ad_creative_bodies'])\n",
        "X_test_suggestion = vectorizer.transform(test_data['ad_creative_bodies'])\n",
        "\n",
        "# Train a regression model\n",
        "gb_regressor_suggestion = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor_suggestion.fit(X_train_suggestion, train_data['cpi'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PARTIAL DEPENDENCE PLOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose a feature for the Partial Dependence Plot (e.g., the first feature)\n",
        "feature_index = 1\n",
        "feature_name = vectorizer.get_feature_names_out()[feature_index]\n",
        "\n",
        "# Create the Partial Dependence Plot.\n",
        "pdp_plotter = pdpbox.pdp.PDPIsolate(gb_regressor_suggestion, pd.DataFrame.sparse.from_spmatrix(X_test_suggestion, columns = vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out(), feature_name, \"zattiri-zort\", n_classes=0)\n",
        "pdp_plot = pdp_plotter.plot()\n",
        "pdp_plot[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SHAP VALUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "def shap_value_plot(shap_values_instance):\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(feature_names, shap_values_instance[0], color='lightblue')\n",
        "    plt.title('SHAP values for instance index {}'.format(instance_index))\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('SHAP value')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "explainer = shap.TreeExplainer(gb_regressor_suggestion)\n",
        "\n",
        "instance_index = 166  # Index of the instance in the test set\n",
        "\n",
        "single_instance = X_test_suggestion[instance_index].toarray()\n",
        "\n",
        "# Calculate SHAP values for the single instance\n",
        "shap_values_single = explainer.shap_values(single_instance)\n",
        "\n",
        "# Extract the SHAP values for the instance\n",
        "shap_values_instance = shap_values_single[0] if isinstance(shap_values_single, list) else shap_values_single\n",
        "\n",
        "shap_value_plot(shap_values_instance)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
